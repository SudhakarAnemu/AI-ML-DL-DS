{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04 -Word2Vec.ipynb","provenance":[],"collapsed_sections":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z-D63ddOQRmW"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>\n","\n","\n","# Word2Vec \n","\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gJ558yRMwpUd"},"source":["## Table of Contents\n","\n","1. [Word2Vec Architecture](#section1)<br>\n","  1.2.[Continuous Bag of Words](#section2)<br>\n","  1.3. [Skip Gram](#section3)\n","2. [Generating Word Vectors using Word2Vec](#section4)<br>\n","    - 2.1 [Importing necessary Libraries](#section401)<br>\n","    - 2.2 [Ignoring Warning messages](#section402)<br>\n","    - 2.3 [Importing the datafile](#section403)<br>\n","    - 2.4 [Iterating through each sentence through the file](#section404)<br>\n","    - 2.5[Creating a CBOW model](#section405)<br>\n","    - 2.6[Creating a Skip Gram model](#section406)<br>\n","    - 2.7[Output](#section407)<br>\n","3. [Applcations](#section408)<br>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Vof5FSa6w2BZ"},"source":["\n","\n","### **Word2Vec** is an algorithm for constructing vector representations of words, also known as word embeddings.\n","\n","<center><img src =\" https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/22344.PNG\"/></center>\n","\n","**Word2Vec** takes care of 2 things:\n","\n","1. It converts high dimensional vector into low dimensional vector.\n","2. Maintains the word context - meaning.\n","\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/meaning.JPG\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QLCiN4AlQRmd"},"source":["**Word2Vec is one of the most widely used models to produce *word embeddings*. The models are shallow, 2 layer neural networks that are trained to reconstruct linguistic context of the word**.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IegBB4aAQRmg"},"source":["**Layers --> Input layer + Hidden layer = Output layer**\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/hidden.PNG\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cGyalx5jQRmi"},"source":["# Word2Vec has 2 architectures :\n","\n","## 1. CBOW (Continuous Bag of Words) :\n","\n","It learns to predict the word by context.\n","\n","Input  --> the context (neighboring words)\n","\n","Output --> target word\n","\n","The limit on the number of words in each context is determined by a parameter called **“window size”**.\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/cbow.png\"/></center>\n","\n","**In the above example-**\n","\n","**INPUT Layer  : White box content**\n","\n","**TARGET Layer : Blue box content**\n","\n","**Window Size  : 5**\n","\n","____\n","\n","## 2. SKIP Gram\n","\n","Skip Gram is learning to predict the context by the word.\n","\n","Input  --> Word\n","\n","Output --> Target Context (neighboring words)\n","\n","The limit on the number of words in each context is determined by a parameter called **“window size”.**\n","\n","![](https://cdn-images-1.medium.com/max/1200/1*5ugorDZ6nOgSqQq1dirY8Q.png)\n","\n","**In the above example-**\n","\n","**INPUT Layer  : Blue box content**\n","\n","**TARGET Layer : White box content**\n","\n","**Window Size  : 5**\n","\n","___"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kRrvTuHJQRml","toc-hr-collapsed":true},"source":["# A simple example in Python to generate word vectors using Word2Vec\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/python.PNG\"/></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O8Bz3LmSQRmo"},"source":["Run these 2 commands in the terminal :\n","\n","```python\n","pip install nltk\n","pip install gensim\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vVN-l4YyQRmr"},"source":["## 2.1 Importing necessary libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":2080,"status":"ok","timestamp":1566814982997,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"Sq-Nr5uFQRmt","outputId":"67dc69c0-7777-43d6-ef76-7e7a6e73a4f7","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","import warnings \n","import gensim \n","from gensim.models import Word2Vec\n","import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M48yw3CjQRm1"},"source":["## 2.2 Ignoring warning messages\n","\n","<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/errors.JPG\" height = \"300\"/></center>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yITVp8ueQRm4","colab":{}},"source":["warnings.filterwarnings(action = 'ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kY93wHs7QRm9"},"source":["## 2.3 Importing the data file (Alice.txt)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wA_xzRmRbjvu","colab":{}},"source":["import urllib\n","sample = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/Alice.txt')\n","s = sample.read().decode('utf8')\n","f = s.replace(\"\\n\", \" \")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sW6OpLbIQRnH"},"source":["## 2.4 Iterating through each sentence in the file"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1eErGUe7QRnK","colab":{}},"source":["data = []\n","\n","for i in sent_tokenize(f): \n","    temp = [] \n","      \n","    # tokenize the sentence into words \n","    for j in word_tokenize(i): \n","        temp.append(j.lower()) \n","  \n","    data.append(temp) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f6w8OWPtQRnS"},"source":["## 2.5 Creating a CBOW model"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1181,"status":"ok","timestamp":1566815044664,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"qs2IAamIQRnV","outputId":"857b4126-de27-4f1f-dd72-1340f748c2cf","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["model1 = gensim.models.Word2Vec(data, min_count = 1,  \n","                              size = 100, window = 5) \n","\n","# Print results \n","print(\"Cosine similarity between 'alice' \" + \n","               \"and 'wonderland' - CBOW : \", \n","    model1.similarity('alice', 'wonderland')) \n","      \n","print(\"Cosine similarity between 'alice' \" +\n","                 \"and 'machines' - CBOW : \", \n","      model1.similarity('alice', 'machines')) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.9995325\n","Cosine similarity between 'alice' and 'machines' - CBOW :  0.95275563\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rH_x3fG7QRng"},"source":["## 2.6 Creating a Skip Gram model"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1422,"status":"ok","timestamp":1566815047721,"user":{"displayName":"Venkatesh Sundaraneedi","photoUrl":"","userId":"00625080329152564246"},"user_tz":-330},"id":"F5t1tLC1QRnh","outputId":"4d5ac4ac-f61f-4dd8-bf4e-79cfe178a7f9","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n","                                             window = 5, sg = 1) \n","  \n","# Print results \n","print(\"Cosine similarity between 'alice' \" +\n","          \"and 'wonderland' - Skip Gram : \", \n","    model2.similarity('alice', 'wonderland')) \n","      \n","print(\"Cosine similarity between 'alice' \" +\n","            \"and 'machines' - Skip Gram : \", \n","      model2.similarity('alice', 'machines')) "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.89742833\n","Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.86560345\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5wzTkQZxQRns"},"source":["## 2.7 Output \n","\n","**Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.99913293**\n","\n","**Cosine similarity between 'alice' and 'machines' - CBOW :  0.98022455**\n","\n","**Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.89401996**\n","\n","**Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.85758847**\n","\n","Output indicates the cosine similarities between word vectors **‘alice’, ‘wonderland’** and **‘machines’** for different models."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SSxdeRq9QRnv"},"source":["## 3. Applications of Word Embeddings:\n","\n","1. Sentiment Analysis\n","\n","2. Speech Recognition\n","\n","3. Information Retrieval\n","\n","4. Question Answering"]}]}