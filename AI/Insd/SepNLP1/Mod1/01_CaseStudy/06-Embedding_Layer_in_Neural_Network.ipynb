{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06-Embedding_Layer_in_Neural_Network.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DJ0gKr7gydy4"},"source":["<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"360\" height=\"160\" /></center>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1ekKNBj8yklB"},"source":["## Embedding Layer in Neural Network "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"smRX8ZV8yoWo"},"source":["## Table of Contents\n","\n","1. [What is word embedding?](#section1)<br>\n","     1.1 [Why You Need to Start Using Embedding Layers](#section101)<br>\n","     1.2 [How to obtaion word embedding](#section102)<br>\n","2. [Learning Word Embeddings for IMDB Movie Review Data](#section2)<br>\n","     2.1 [Imdb Movie Review data](#section201)<br>\n","      - 2.1.1 [Overview](#section20101)<br>\n","      - 2.1.2 [Dataset](#section20102)<br>\n","3. [Instantiating an Embedding layer](#section3)<br>\n","     - 3.1 [Loading the IMDB data for use with an Embedding layer](#section301)<br>\n","     - 3.2 [Let's Train the model](#section302)<br>\n","     \n","4. [Conclusion](#section4)<br>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i4ZHZ-TNyrKW"},"source":["<a id=\"section1\"></a>\n","## 1. What is word embedding?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pOj27OD5yu3f"},"source":["A __word embedding__ is a learned representation for text where words that have the same meaning have a similar representation.__Word embeddings__ provide a dense representation of words and their relative meanings.\n","\n","Because of word embedding words of similar context are close to each other.\n","<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/Word-Vectors.png\"/>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2ECg_UWRyyXi"},"source":["### 1.1 Why You Need to Start Using Embedding Layers?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WqqAeMiby1h-"},"source":["So why should you use an __embedding layer__? Here are the two main reasons:\n","\n","- __One-hot encoded vectors__ are high-dimensional and sparse. Let’s assume that we are doing __Natural Language Processing (NLP)__ and have a dictionary of __2000 words__. This means that, when using __one-hot encoding__, each word will be represented by a vector containing __2000 integers__. And __1999__ of these integers are __zeros__. In a big dataset this approach is not computationally efficient.\n","\n","- The vectors of each __embedding__ get updated while training the neural network. If you have seen the image at the top of this post you can see how __similarities__ between words can be found in a __multi-dimensional space__. This allows us to visualize __relationships between words__, but also between everything that can be turned into a __vector__ through an __embedding layer__."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m45uhOJ2y_Uk"},"source":["### 1.2 How to obtain word embedding?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tm3vXCv3zC-L"},"source":["There are multiple ways to obtain __word embeddings__ few are here:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AjmwKpAwzFoa"},"source":["- Learn __word embeddings__ jointly with the main task you care about(such as document __classification or sentiment prediction__). In this setup, you start with __random word vectors__ and then learn __word vector__ in the same way you learn the __weight__ of a __neural network__.\n","- Load into your model __word embedidng__ that were precomputed using different machine-learning task tha the one you're trying to solve. These are called __pretrained word embeddings__\n","    Two popular word embeddings are\n","    - __GloVe__ \n","    - __fastText__\n","    \n","- Word Embeddings with __Gensim__"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kFa7tgMszIzP"},"source":["<a id=\"section2\"></a>\n","## 2. Learning Word Embeddings for IMDB Movie Review Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vrmBCVq9_P6B"},"source":["<a id=\"section201\"></a>\n","### 2.1 Imdb Movie Review data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d5z32Du5zMGH"},"source":["<a id=\"section20101\"></a>\n","#### 2.1.1 Overview\n","This dataset contains __movie reviews__ along with their associated binary __sentiment polarity labels__. It is intended to serve as a benchmark for __sentiment classification__. This document outlines how the dataset was gathered, and how to use the files provided.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qyrUA3Kd_Yzy"},"source":["<a id=\"section20102\"></a>\n","#### 2.1.2 Dataset\n","The core dataset contains __50,000 reviews__ split evenly into __25k train and 25k test sets__. The overall distribution of labels is balanced (__25k pos__ and __25k neg__).\n","\n","In the entire collection, no more than __30 reviews__ are allowed for any given movie because reviews for the same movie tend to have __correlated ratings__. Further, the __train and test__ sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels. In the labeled __train/test__ sets, a __negative review__ has a __score <= 4__ out of __10__, and a __positive review__ has a __score >= 7__ out of __10__. Thus reviews with more __neutral ratings__ are not included in the __train/test__ sets. In the unsupervised set, reviews of any rating are included and there are an even number of reviews > 5 and <= 5."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zr6WqhgVzPIQ"},"source":["<a id=\"section3\"></a>\n","## 3.  Instantiating an Embedding layer"]},{"cell_type":"code","metadata":{"id":"ItkIWd-QuyJn","colab_type":"code","outputId":"b56cdedf-e2a7-42a4-f80b-bbcfd4143a33","executionInfo":{"status":"ok","timestamp":1579503309628,"user_tz":-330,"elapsed":1943,"user":{"displayName":"Mohit Raj","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWeTZUrbhhCiZ-3HbrA91YVJTlW3olUgVSeOr8Ww=s64","userId":"17943522262909727589"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Import tensorflow 2.x\n","# This code block will only work in Google Colab.\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eZd8wRYizPlT","colab":{}},"source":["from tensorflow.keras.layers import Embedding\n","embedding_layer = Embedding(1000, 64)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wNUAxux50k5C"},"source":["<a id=\"section301\"></a>\n","### 3.1 Loading the IMDB data for use with an Embedding layer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u82mwFdksk2A"},"source":["Here we first need to __install__ the required version of __numpy__ for __keras__. Because the current numpy version is not __compatible__ with __keras__.\n","\n","Until then, try __downgrading__ your numpy version to __1.16.2.__ It seems to solve the problem.\n","\n","If your current version of numpy works properly with the upcoming code, then there's no need to downgrade it."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xny7wHYM4uHy","colab":{}},"source":["# !pip install numpy==1.16.1\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1DyOWZ_-vVG1","colab":{}},"source":["from tensorflow.keras.datasets import imdb\n","from tensorflow.keras import preprocessing"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0VgsV2nVvYKB"},"source":[" - We will consider __10000 most common words__"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Owtx-7T9vtAu","colab":{}},"source":["max_features = 10000  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uw75On7Xvtqv"},"source":[" - We will consider only __first 20 words__ for each __movie review__\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gk-f5Y8bvzJA","colab":{}},"source":["maxlen = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tOkGhKicwJFs"},"source":["- Load the data as __lists of integers__, which is already done in keras "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-p4SZTHgwWV_","colab":{}},"source":["(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6tYb1w8YwaQq"},"source":["- Transform the lists of integers into a __2D integer tensor__ of shape (__samples, maxlen__)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cCEQ5Au_zR36","colab":{}},"source":["x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)  \n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q3jjYuMQ3CwD"},"source":["- Print the __shape__ for both __train__ and __test__ data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wg0V4M980rqX","outputId":"39a48b75-4243-411a-9806-7e41fc8c38a7","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(x_train.shape)\n","\n","print(x_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(25000, 20)\n","(25000, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6zuDGzNX6He_"},"source":["<a id=\"section302\"></a>\n","### 3.2 Let's Train the model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s77_gxBkww4d"},"source":["- Import __keras sequential api__ to create models __layer-by-layer__"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ptahnUPowu55","colab":{}},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","model = Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CdlILM8Xxjey"},"source":["- Specifies the maximum input length to the __Embedding layer__ so you can later flatten the embedded inputs. After the __Embedding layer__, the activations have shape __(samples, maxlen, 8)__\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gN9yY19yxe8L","colab":{}},"source":["model.add(Embedding(10000, 8, input_length=maxlen))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lm0aXsxS0Qz1"},"source":["-  __Flatten__ layer in Keras "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"018hjSl70TUD","colab":{}},"source":["model.add(Flatten())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qmtzaGrx0VkI"},"source":["- Adds the __classifier__ on top"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"28BIbo2w0hGj","colab":{}},"source":["model.add(Dense(1, activation='sigmoid'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G7x_gf8J1Fcg"},"source":["#### What does compile do?\n","\n","- __Compile__ defines the __loss function__, the optimizer and the metrics. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"alEH1XBh1TJ0","colab":{}},"source":["model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TMPslS1u1qtE"},"source":["- Print the __summary representation__ of your model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"emmpu5L21w0X","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rmIab8GC15On"},"source":["- Train the model on your __custom movie data__."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JrhToVYG5TYu","outputId":"c31f7764-645a-4641-c7a3-5a8b8fcf6f15","colab":{"base_uri":"https://localhost:8080/","height":836}},"source":["history = model.fit(x_train, y_train,epochs=10,batch_size=32,validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0827 07:15:19.396104 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0827 07:15:19.415315 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0827 07:15:19.463059 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0827 07:15:19.483021 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0827 07:15:19.489022 139633655900032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 160)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["W0827 07:15:19.725079 139633655900032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n","20000/20000 [==============================] - 2s 106us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6812\n","Epoch 2/10\n","20000/20000 [==============================] - 1s 63us/step - loss: 0.5658 - acc: 0.7425 - val_loss: 0.5467 - val_acc: 0.7204\n","Epoch 3/10\n","20000/20000 [==============================] - 1s 60us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n","Epoch 4/10\n","20000/20000 [==============================] - 1s 59us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n","Epoch 5/10\n","20000/20000 [==============================] - 1s 61us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n","Epoch 6/10\n","20000/20000 [==============================] - 1s 59us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n","Epoch 7/10\n","20000/20000 [==============================] - 1s 58us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n","Epoch 8/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n","Epoch 9/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.3022 - acc: 0.8767 - val_loss: 0.5213 - val_acc: 0.7492\n","Epoch 10/10\n","20000/20000 [==============================] - 1s 57us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GGQufrV7-E7m"},"source":["<a id=\"section4\"></a>\n","## 4. Conclusion"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BDokqHFl824u"},"source":["- You get to a __validation accuracy__ of __~76%__, which is pretty good considering that you’reonly looking at the first __20 words__ in every review. \n","\n","- But note that merely flattening the __embedded sequences__ and training a __single Dense layer__ on top leads to a model that treats each word in the input sequence separately, without considering __inter-word relationships__ and __sentence structure__ (for example, this model would likely treat both “this movie is a bomb” and “__this movie is the bomb__” as being __negative reviews__). \n","-\n","It’s much better to add __recurrent layers__ or __1D convolutional layers__ on top of the embedded sequences to learn features that take into account each sequence as a whole."]}]}